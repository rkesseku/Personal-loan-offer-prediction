---
title: "Bank customer management"
author: "Reagan Kesseku"
date: "2018-06-02"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE}
# set global chunk options
# -------------------------
knitr::opts_chunk$set(echo = TRUE, cache = F, tidy = T,
                      warning = F, message = F, options(digits = 3))
```

```{r}
# set working directory
# ---------------------
setwd("D:/Ph.D_materials/Programming/R_programming/mdsr/Loan offers")
```

```{r}
# Load functions and packages
# ---------------------------
source("bank_pkg.R")
```

## DATA VISUALIZATION 





```{r}
# Import the Mall bank data 
# --------------------------------
bank <-  read.csv("Bank_Personal_Loan_Modelling.csv", header =  T, 
                  stringsAsFactors = T, colClasses = col_classes
                  )
dim(bank)
datatable(sample_n(bank, 10))  # take a sample view of the imported data
```


```{r}
# rename certain variables for easy call
# --------------------------------------
bank <- bank %>% rename(
  zip_code = 'ZIP.Code',
  personal_loan = 'Personal.Loan',
  securities_acc = 'Securities.Account',
  cd_account = 'CD.Account'
  )

# Change personal loan to factor 
# ------------------------------
bank <- bank %>% mutate(
  personal_loan = factor(personal_loan)
)
```

There are `r dim(bank)[1]` observations and `r dim(bank)[2]` variables in the movies data. Additionally, all variables were numerical. However, we convert the class variables to factors.


#### EXPLORATORY DATA ANAYSIS
```{r}
# check for missing values
miss_check <- function(x) {
  res <- apply(x, 2, function(col){sum(is.na(col))})
  return(res)
}

miss_check(bank) %>% as.data.frame()
```

```{r}
# check the five number summary and other measures of income
# ----------------------------------------------------------
d <- favstats(Income ~ Education, data = bank)

d
knitr::kable(d, digits = 3, 
             format.args = list(scientific = FALSE),
              caption = 'Descriptive summary of age by gender.')
```



```{r , personal-loan}
data_counts <- bank %>% count(personal_loan)
data_percentages <- data_counts %>%
      mutate(percentage = n / sum(n) * 100)

# Create the bar chart
ggplot(data_percentages, aes(x = personal_loan, y = percentage, fill = personal_loan)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(n, "\n", "(", percentage, "%",")")), fontface = "bold",
            vjust = 1.3, size = 3, color = "black") +
  scale_fill_brewer(palette = "Set2")+
  labs(y = "Frequency", 
       x = "Personal Loan",
       title = "Bar chart showing personal loan acceptance and rejection"
       ) + 
  theme_bw() 
```
Clearly, this shows a highly imbalanced classification problem.

```{r , family size}
data_counts <- bank %>% count(Family)
data_percentages <- data_counts %>%
      mutate(percentage = n / sum(n) * 100) %>%
  arrange(desc(percentage))  # Order in ascending 

# Order the levels of Family based on ascending order of frequency
data_percentages$Family <- factor(data_percentages$Family, levels = data_percentages$Family[order(data_percentages$n)])

# Create the bar chart
ggplot(data_percentages, aes(x = Family, y = percentage, fill = factor(Family))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(n, "\n", "(", percentage, "%",")")), 
            fontface = "bold", vjust = 2.0, 
            size = 3, color = "black") +
  scale_fill_brewer(palette = "Set3")+
  labs(y = "Frequency", 
       x = "Family size",
       fill = "Family size",
       title = "Distribution of family size of bank customers"
       ) + 
  theme_bw() 
```

```{r "density-plot-Annual_income-gender"}
# calculate median income and spending across gender 
mu_income <- plyr::ddply(bank, "personal_loan", summarise, grp.median = median(Income))

# plot graph
# -----------
 p1 <- ggplot(data = bank, aes(x = Income, color = personal_loan, fill =  personal_loan)) +
  geom_density(alpha = 0.3) +
  geom_vline(data = mu_income, aes(xintercept = grp.median, color = personal_loan),
             linetype = "dashed") +
 scale_fill_manual(values = c("#E7B800", "#00AFBB"))+
  labs(title = "Annual income distribution across personal loan",
       x = "Income") +
  theme_bw() +
  facet_wrap(~ Education, nrow = 1, 
             labeller = labeller(Education = c("1" = "Undergrad", 
                                               "2" = "Graduate", 
                                               "3" = "Professional")))

p1
```

```{r}
# find the unique zip codes
bank %>% select(zip_code) %>% unique() %>% nrow()
```

```{r}
# Create a density plot
ggplot(data = bank, aes(x = Mortgage)) +
  geom_density(aes(fill = factor(cd_account)), alpha = 0.5) +
  labs(x = "Mortgage", y = "Density", 
       title = "Distribution of Mortage for Certificate Deposit Account",
       fill = "Certificate \n Deposit \n Account") +
  theme_bw()

```

```{r}
# removing certain variables before prediction
# --------------------------------------------
bank1 <- bank %>% select(-ID, -zip_code)
# bank1$personal_loan <- bank1$personal_loan %>% as.factor()

glimpse(bank)

```

```{r}
# Partitioning the data
#----------------------
seed = 123  
set.seed(seed)  # for reproducibility
class_split <- initial_split(bank1, prop = 0.75, strata = "personal_loan")
train <- training(class_split)
test <- testing(class_split)
```

\subsection{selecting response and predictor variables}

```{r}
# scaling income in train data
# --------------------------
train_income_scale <- scale(train$Income)

# scaling income with train scale values to prevent data leakage
# ---------------------------------------------------------------
test_income_scale = scale(test$Income, 
      center = attr(train_income_scale, "scaled:center"),
      attr(train_income_scale, "scaled:scale"))

# Replace income values 
train <- train %>% 
  mutate(Income = as.vector(train_income_scale))

test <- test %>% 
  mutate(Income = as.vector(test_income_scale))


# Repeating for mortgage
# ------------------------
train_Mortgage_scale <- scale(train$Mortgage)
test_Mortgage_scale = scale(test$Mortgage, 
                          center = attr(train_Mortgage_scale, "scaled:center"),
                          attr(train_Mortgage_scale, "scaled:scale"))

# Replace Mortgage values 
train <- train %>% 
  mutate(Mortgage = as.vector(train_Mortgage_scale))

test <- test %>% 
  mutate(Mortgage = as.vector(test_Mortgage_scale))
# ----------------------------------------------

train_y <- train$personal_loan                   # response    
train_x <- train %>% select(-personal_loan)      # predictors

test_x <- test %>% select(-personal_loan)      # predictors
test_y <- test$personal_loan                   # response
```

```{r}
# Multiple Logistic regression model
# ----------------------------------
logistic <- train(personal_loan ~ .,
data = train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 3)
)

summary(logistic)
```

```{r}
# Logistic model prediction accuracy
# ----------------------------------
pred_logistic <- predict(logistic, test, type = "prob")

roc_logistic <- PRROC::roc.curve(scores.class0 = pred_logistic[ ,2][test$personal_loan == 1], 
               scores.class1 = pred_logistic[ ,2][test$personal_loan == 0], 
               curve = T, rand.compute = T)
plot(roc_logistic, rand.plot = T, main = "ROC curve for MLR")


# PR Curve
pr_logistic <- pr.curve(scores.class0 = pred_logistic[,2][test$personal_loan == 1], 
              scores.class1 = pred_logistic[,2][test$personal_loan == 0], 
              curve = T, rand.compute = T)
plot(pr_logistic, rand.plot = T, main = "PR curve for MLR")
```



```{r}
# Using XGBoost on RAW TRAIN
# ----------------------
# creating data matrices

train_y <- as.numeric(train$personal_loan) 
train_y <- ifelse(train_y == 1, 0, 1)
test_y <- as.numeric(test$personal_loan) 
test_y <- ifelse(test_y == 1, 0, 1)


xgb_train = xgb.DMatrix(data = data.matrix(train_x), label = train_y)
xgb_test = xgb.DMatrix(data = data.matrix(test_x), label = test_y)
```

```{r}
set.seed = 123
# Set XGBoost parameters
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 6,
  eta = 0.3,
  verbose = T,
  nthread = 4)

# Train the XGBoost model
# xgb_model <- xgboost(data = xgb_train, params = params, nrounds = 100, verbose = F)
```

```{r}
# Final model fit
# ----------------
set.seed(seed)
xgb_model = xgboost(data = xgb_train, max.depth = 6, nthread = 6, 
                  eta = 0.01, nrounds = 200, subsample = 0.5, gamma = 0,
                  min_child_weight = 1, booster = "gbtree", 
                  scale_pos_weight = sum(train_y == 0)/sum(train_y == 1),
                  objective = "binary:logistic", verbose = 0)


# Performing prediction
# ---------------------
pred_morb_xgb = predict(xgb_model, xgb_test, type = "response")

# ROC Curve    
xgb_roc <- PRROC::roc.curve(scores.class0 = pred_morb_xgb[test_y == 1], 
               scores.class1 = pred_morb_xgb[test_y == 0], 
               curve = T, rand.compute = T)

plot(xgb_roc, rand.plot = T, main = "ROC curve for Xgboost on train")

# PR Curve
xgb_pr <- PRROC::pr.curve(scores.class0 = pred_morb_xgb[test_y == 1], 
              scores.class1 = pred_morb_xgb[test_y == 0], 
              curve = T, rand.compute = T)

plot(xgb_pr, rand.plot = T, main = "PR curve for Xgboost on train")
```

```{r}
train2 <- train
train2$personal_loan <- ifelse(train2$personal_loan == "0", "declined", "accepted")
test2_y <- test_y
test2_y <- ifelse(test2_y == "0", "declined", "accepted")
```


```{r}
# parallel compute
# ------------------
cl = makePSOCKcluster(5)
registerDoParallel(cl)

# Fitting KNN model
# ==================
cv <- trainControl(method = "repeatedcv", 
                   number = 10, 
                   repeats = 5, 
                   classProbs = TRUE, 
                   summaryFunction = twoClassSummary
)

# Create a hyperparameter grid search
hyper_grid <- expand.grid(k = floor(seq(1, 
                              nrow(train2)/80, 
                              length.out = 35)))


# Fit knn model and perform grid search
knn_fit <- train(factor(personal_loan) ~.,  
                data = train2,  
                preProc = c("center", "scale"),
                method = "knn",
                trControl = cv,
                tuneGrid = hyper_grid,
                metric = "ROC"
)

ggplot(knn_fit)
```
```{r}
# print model results
knn_fit
```
Result shows the optimal value occurs at k = 34

```{r}
# check the performance at the optimal value (k = 34)
knn_fit$results[knn_fit$results$k == 32, ]
```

```{r}
# prediction on test data
# -----------------------
pred.knn <- predict(knn_fit, test)

# ROC Curve
roc_knn <- PRROC::roc.curve(scores.class0 = pred.knn[test2_y == "declined"],
               scores.class1 = pred.knn[test2_y == "accepted"],
               curve = T, rand.compute = T)

plot(roc_knn, rand.plot = T, main = "ROC curve for kNN on train")


# PR Curve
pr_knn <- pr.curve(scores.class0 = pred.knn[test2_y == "declined"],
               scores.class1 = pred.knn[test2_y == "accepted"],
               curve = T, rand.compute = T)
plot(pr_knn, rand.plot = T, main = "PR curve for kNN on train")
```
```{r}
# Control params for SVM
ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary # also needed for AUC/ROC
)


# Tune an SVM
set.seed(123) # for reproducibility
svm_fit <- train(
personal_loan ~ .,
data = train2,
method = "svmRadial",
preProcess = c("center", "scale"),
metric = "ROC", # area under ROC curve (AUC)
trControl = ctrl,
tuneLength = 15
)
```


```{r}
# Plot results
ggplot(svm_fit) + theme_light()
```


```{r}
# prediction on test data
# -----------------------
pred.svm <- predict(svm_fit, test)

# ROC Curve
roc_svm <- PRROC::roc.curve(scores.class0 = pred.svm[test2_y == "declined"],
               scores.class1 = pred.svm[test2_y == "accepted"],
               curve = T, rand.compute = T)

plot(roc_svm, rand.plot = T, main = "ROC curve for SVM on train")


# PR Curve
pr_svm <- pr.curve(scores.class0 = pred.svm[test2_y == "declined"],
               scores.class1 = pred.svm[test2_y == "accepted"],
               curve = T, rand.compute = T)
plot(pr_svm, rand.plot = T, main = "PR curve for SVM on train")
```
```{r}
# Random Forest model
# ===================
set.seed(123)
# train a default random forest model
rf_model <- ranger(personal_loan ~ .,
                  data = train,
                  mtry = floor(ncol(train_x)/ 3),# using the rule of thumb
                  importance='permutation',
                  num.trees = 6000,
                  max.depth = 12)
```

```{r}
# prediction on test data
# -----------------------
pred.rf <- predict(rf_model, test, type = "response")$predictions



# ROC Curve
roc_rf <- PRROC::roc.curve(scores.class0 = pred.rf[test_y == 1],
               scores.class1 = pred.rf[test_y == 0],
               curve = T, rand.compute = T)

plot(roc_rf, rand.plot = T, main = "ROC curve for Random forest on train")


# PR Curve
pr_rf <- pr.curve(scores.class0 = pred.rf[test_y == 0],
               scores.class1 = pred.rf[test_y == 1],
               curve = T, rand.compute = T)
plot(pr_rf, rand.plot = T, main = "PR curve for Random forest train")
```


```{r}
# PNG device
png("roc_pr.png", width = 950, height = 500,
    units = "px", pointsize = 12, bg = "white", res = NA,
    restoreConsole = TRUE)

par(mfrow = c(1, 2))
# plotting overlaid ROC curves
# ----------------------------
plot(roc_logistic, main = "ROC-AUC for classification models", rand.plot = T,
     col = "black", lwd = 2.5, auc.main = F)
plot(xgb_roc,  lty = 1, add = TRUE, col = "cyan", lwd = 2.5,  auc.main = F)
plot(roc_knn, lty = 1 , add = TRUE, col = "tan2", lwd = 2.5,  auc.main = F)
plot(roc_rf, lty = 1 , add = TRUE, col ="red", lwd = 2.5,  auc.main = F)
plot(roc_svm, lty = 1 , add = TRUE, col ="green2", lwd = 2.5,  auc.main = F)
legend(x = "bottomright",          # Position
       legend = c(paste("MLR =", round(roc_logistic$auc, 3), sep = ""), 
                  paste("XGB =", round(xgb_roc$auc, 3), sep = ""), 
                  paste("kNN =", round(roc_knn$auc, 3), sep = ""), 
                  paste("RF =", round(roc_rf$auc, 3), sep = ""), 
                  paste("SVM =", round(roc_svm$auc, 3), sep = "")
                  ),  # Legend texts
          lty = c(1, 1, 1, 1, 1),           # Line types,           # Line types
          col = c("black", "darkblue", "tan2", "red", "green2"),     # Line colors
          lwd = 2)

xgb_roc$auc

# plotting overlaid PR curves
# ----------------------------
plot(pr_logistic, main = "PR-AUC for classification models", rand.plot = T,
                        col = "black", lwd = 2.5,  auc.main = F, plot = T)
plot(xgb_pr, lty = 1 , add = TRUE, col = "cyan", lwd = 2.5,  auc.main = F)
plot(pr_knn, lty = 1 , add = TRUE, col ="tan2", lwd = 2.5,  auc.main = F)
plot(pr_rf,  lty = 1, add = TRUE, col = "red", lwd = 2.5,  auc.main = F)
plot(pr_svm,  lty = 1, add = TRUE, col = "green2", lwd = 2.5,  auc.main = F)
legend(x = "bottomleft",          # Position
       legend = c(paste("MLR =", round(pr_logistic$auc.integral, 3), sep = ""), 
                  paste("XGB =", round(xgb_pr$auc.integral, 3), sep = ""), 
                  paste("kNN =", round(pr_knn$auc.integral, 3), sep = ""), 
                  paste("RF =", round(pr_rf$auc.integral, 3), sep = ""), 
                  paste("SVM =", round(pr_svm$auc.integral, 3), sep = "")),  # Legend texts
          lty = c(1, 1, 1, 1, 1),           # Line types,           # Line types
          col = c("black", "cyan", "tan2", "red", "green2"),              # Line colors
          lwd = 2) 

dev.off()
```
Clearly, the best model for the data is XGBoost

### Variable importance from the optimal model

```{r pdf8, message = F, echo=FALSE, fig.align = 'top', dev='pdf', fig.height=7, fig.width=12, fig.cap = 'Correlation plot.'}
# Variable Importance for best model
# ---------------------------------
png("xgb_varimp.png", width = 750, height = 520,
    units = "px", pointsize = 12, bg = "white", res = NA,
    restoreConsole = TRUE)

importance_matrix <- importance_matrix %>%
  mutate(Feature = factor(Feature, levels = Feature[order(Gain)]))

# Create the bar chart
ggplot(importance_matrix, aes(x = Feature, y = Gain, fill = factor(Feature))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(Gain)), 
            fontface = "bold", hjust = 1.0, 
            size = 3, color = "black") +
  scale_fill_brewer(palette = "Set3")+
  labs(y = "Variable Importance", 
       x = "Predictors",
       fill = "Predictors",
       title = " "
       ) + 
  theme_bw() +
  coord_flip()
```

